{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8_Sentinel2_cropping\n",
    "### Crops the Sentinel-2 roof image of buildings in the curated labelled data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial configuration\n",
    "#### To start working with this particular notebook, you need to provide necessary credential and settings\n",
    "#### Below is an template of configuration, which is necessary prepare aside of this notebook and copy & paste all content in triple quotes to the next cell's input field\n",
    "    \"\"\"\n",
    "    {\n",
    "    \"COS_ENDPOINT_URL\": \"s3.private.eu-de.cloud-object-storage.appdomain.cloud\",\n",
    "    \"COS_AUTH_ENDPOINT_URL\": \"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    \"COS_APIKEY\": \"xxx\",\n",
    "    \"BUCKET_TIFF\": \"kenya-images\",\n",
    "    \"DATA_CURATION_BUCKET\": \"xxx\"\n",
    "    }\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read notebook configuration\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "config_str = getpass.getpass('Enter your prepared config: ')\n",
    "config = json.loads(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import io\n",
    "from PIL import Image\n",
    "import ibm_boto3\n",
    "from botocore.client import Config\n",
    "import numpy as np\n",
    "import configparser\n",
    "import os\n",
    "from ibm_cloud_sdk_core import ApiException\n",
    "from ibmcloudant.cloudant_v1 import CloudantV1, Document, BulkDocs\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import time\n",
    "import base64\n",
    "import shutil\n",
    "import threading\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import rasterio\n",
    "from pyproj import Geod\n",
    "from shapely.geometry import Polygon, MultiPolygon, mapping, Point\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External utils succesfully imported\n"
     ]
    }
   ],
   "source": [
    "# initiate the S3 client\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "                              ibm_api_key_id=config[\"COS_APIKEY\"],\n",
    "                              ibm_auth_endpoint=config[\"COS_AUTH_ENDPOINT_URL\"],\n",
    "                              config=Config(signature_version='oauth'),\n",
    "                              endpoint_url=config[\"COS_ENDPOINT_URL\"])\n",
    "\n",
    "\n",
    "response = cos_client.list_objects_v2(Bucket=config[\"UTILS_BUCKET\"])\n",
    "\n",
    "# download utils module\n",
    "try:\n",
    "    from utils import *\n",
    "    print('External utils succesfully imported')\n",
    "    \n",
    "except Exception as e:\n",
    "    print('Desired utils package is missing in local env, downloading it...', e)\n",
    "    for obj in response['Contents']:\n",
    "        name = obj['Key']\n",
    "        streaming_body_1 = cos_client.get_object(Bucket=config[\"UTILS_BUCKET\"], Key=name)['Body']\n",
    "        print(\"Downloading to localStorage :  \" + name)\n",
    "        with io.FileIO(name, 'w') as file:\n",
    "            for i in io.BytesIO(streaming_body_1.read()):\n",
    "                file.write(i)\n",
    "    from utils import *\n",
    "    print('External utils succesfully imported')\n",
    "\n",
    "\n",
    "\n",
    "# initiate the Cloudant client\n",
    "authenticator = IAMAuthenticator(config[\"CLOUDANT_API_KEY\"])\n",
    "client = CloudantV1(authenticator=authenticator)\n",
    "client.set_service_url(config[\"CLOUDANT_URL\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign cinfig necessary variables\n",
    "BUCKET_TIFF = config[\"BUCKET_TIFF\"]\n",
    "labelled_data_SMOD_heights_parquet = 'all_labelled_data_SMOD_heights.parquet'\n",
    "labelled_data_SMOD_heights_sentinel2_parquet = 'all_labelled_data_SMOD_heights_sentinel2.parquet'\n",
    "curation_bucket = config[\"DATA_CURATION_BUCKET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_images_bucket = cos_client.list_objects_v2(Bucket=BUCKET_TIFF)\n",
    "tif_images_objects = tif_images_bucket['Contents']\n",
    "\n",
    "tif_images_filenames = [obj['Key'] for obj in tif_images_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the labelled data set with all info\n",
    "if type(curation_bucket) == str:\n",
    "\n",
    "    streaming_body = cos_client.get_object(Bucket=curation_bucket, Key=labelled_data_SMOD_heights_parquet)['Body']\n",
    "    print(\"Downloading to local storage :  \" + labelled_data_SMOD_heights_parquet)\n",
    "    with io.FileIO(labelled_data_SMOD_heights_parquet, 'w') as file:\n",
    "        for i in io.BytesIO(streaming_body.read()):\n",
    "            file.write(i)\n",
    "\n",
    "buildings_df = gpd.read_parquet(labelled_data_SMOD_heights_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0  # defines how many pixels we add to the building wenn preparing the dataset.\n",
    "# where the preprocessed samples shall be stored shall be stored\n",
    "folder_preprocessed_files = 'samples/'\n",
    "os.makedirs(os.path.dirname(folder_preprocessed_files), exist_ok=True)\n",
    "\n",
    "path_to_tif_folder = 'tiff/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('tiff/', ignore_errors=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for tiff_name in tif_images_filenames: # iterate through grid system\n",
    "    init_time = time.time()\n",
    "    print('Create /tiff directory')\n",
    "    os.makedirs(os.path.dirname(path_to_tif_folder), exist_ok=True)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    \n",
    "    streaming_body = cos_client.get_object(Bucket=BUCKET_TIFF, Key=tiff_name)['Body']\n",
    "    \n",
    "\n",
    "    with io.FileIO(path_to_tif_folder + tiff_name, 'w') as file:\n",
    "        print(\"Copying to localStorage: \" + path_to_tif_folder + tiff_name)\n",
    "        for i in io.BytesIO(streaming_body.read()):\n",
    "            file.write(i)\n",
    "                \n",
    "    print(f'Files downloaded, time took: {time.strftime(\"%H:%M:%S\", time.gmtime(int(time.time() - t1)))}')\n",
    "    t1 = time.time()\n",
    "    \n",
    "    areas_covered_by_tifs = create_bounds_dict(path_to_tifs=path_to_tif_folder)\n",
    "    areas_covered_by_tif = areas_covered_by_tifs[tiff_name]\n",
    "\n",
    "    lon_min = areas_covered_by_tif['lons_sorted'][0]\n",
    "    lon_max = areas_covered_by_tif['lons_sorted'][1]\n",
    "\n",
    "    lat_min = areas_covered_by_tif['lats_sorted'][0]\n",
    "    lat_max = areas_covered_by_tif['lats_sorted'][1]\n",
    "    bbox = {\n",
    "        'lon_min': lon_min,\n",
    "        'lon_max': lon_max,\n",
    "        'lat_min': lat_min,\n",
    "        'lat_max': lat_max\n",
    "    }\n",
    "    print(lon_min, lon_max, lat_min, lat_max)\n",
    "    t1 = time.time()\n",
    "    # df = fetch_builings_in_bbox(lon_min, lon_max, lat_min, lat_max)\n",
    "\n",
    "    df = buildings_df[\n",
    "                (buildings_df.latitude >= lat_min) & \\\n",
    "                (buildings_df.latitude <= lat_max) & \\\n",
    "                (buildings_df.longitude >= lon_min) & \\\n",
    "                (buildings_df.longitude <= lon_max)\n",
    "            ].copy()\n",
    "\n",
    "    # df.index = [i for i in range(len(df))]\n",
    "        \n",
    "    df[\"corresponding_tiff\"] = ['NA' for _ in range(len(df))]\n",
    "#     df['image_name'] = ['' for _ in range(len(df))]\n",
    "#     df['tiff_name'] = ['' for _ in range(len(df))]\n",
    "    tifs = df.corresponding_tiff.unique().tolist()\n",
    "    if len(df) == 0:\n",
    "        print(\"No tiff file was found, that corresponds with Lon and Lat coordinates in GeoDataFrame\")\n",
    "    else:\n",
    "#         for tif in tqdm(tifs, desc =\"TIF files processing:\"):\n",
    "            \n",
    "        # sanity check: is it a valid tif path\n",
    "        tif = path_to_tif_folder + tiff_name\n",
    "        if isinstance(tif, str):\n",
    "            if tif.endswith('.tif'):\n",
    "                with rasterio.open(tif) as dataset:\n",
    "                    bands = dataset.read()\n",
    "                    \n",
    "                    \n",
    "                    # Assuming the TIFF files have 3 bands (RGB)\n",
    "                    if bands.shape[0] == 3:  # Checking if it has 3 bands (R, G, B)\n",
    "                        # Reorder array from 3, height, width to height, width, 3\n",
    "                        picture_all_bands = np.transpose(bands, (1, 2, 0))\n",
    "\n",
    "                        # Convert to RGB\n",
    "                        picture = np.clip(picture_all_bands, 0.0, 255.0).astype('uint8')\n",
    "                        \n",
    "                        current_batch = 0\n",
    "                        images_batch = []\n",
    "\n",
    "                        for index, row in tqdm(df.iterrows(), desc='Cropping images', total=len(df)):\n",
    "                            \n",
    "                            try:\n",
    "                                \n",
    "                                pixel_coordinates = get_pixel_coordinates(row.geometry, areas_covered_by_tifs, dataset)\n",
    "\n",
    "                                rowcolminmax = get_min_max_values_of_row_col(pixel_coordinates=pixel_coordinates)\n",
    "\n",
    "                                label_rgb = picture[rowcolminmax['rowminmax'][0] - margin:rowcolminmax['rowminmax'][1] + margin + 1,\n",
    "                                        rowcolminmax['colminmax'][0] - margin:rowcolminmax['colminmax'][1] + margin + 1, :]\n",
    "\n",
    "                                \n",
    "                                im = Image.fromarray(label_rgb.astype(\"uint8\"))\n",
    "\n",
    "                                rawBytes = io.BytesIO()\n",
    "                                im.save(rawBytes, \"png\")\n",
    "                                rawBytes.seek(0)\n",
    "\n",
    "                                # df.at[index, 'image_name'] = f\"{index}.png\"\n",
    "\n",
    "                                df.at[index, 'tiff_name'] = tiff_name\n",
    "                                df.at[index, 'image_source_bytes'] = base64.b64encode(rawBytes.read()).decode('ascii')\n",
    "\n",
    "                                \n",
    "\n",
    "                            # save_sample(rasterio_bands_transformed=label_rgb, folder_to_store=folder_preprocessed_files, image_name=f\"{index}.png\")\n",
    "                            # print(f'Img saved: {index}.png')\n",
    "                                    \n",
    "                            except Exception as e:\n",
    "                                print('Image processing error:', e)\n",
    "                                # print(traceback.format_exc())\n",
    "        \n",
    "        dfs.append(df)\n",
    "        \n",
    "    print('Remove tiff/ directory')\n",
    "    shutil.rmtree('tiff/', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.concat(dfs)\n",
    "main_df = main_df.drop_duplicates(subset='geometry')\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_parquet(labelled_data_SMOD_heights_sentinel2_parquet)\n",
    "\n",
    "# optionaly upload file to the bucket\n",
    "if type(curation_bucket) == str:\n",
    "        \n",
    "    try:\n",
    "        cos_client.upload_file(\n",
    "            Filename=labelled_data_SMOD_heights_sentinel2_parquet,\n",
    "            Bucket=curation_bucket,\n",
    "            Key=labelled_data_SMOD_heights_sentinel2_parquet,\n",
    "            ExtraArgs={'ContentDisposition': 'attachment'}\n",
    "        )\n",
    "           \n",
    "        print(f'File {labelled_data_SMOD_heights_sentinel2_parquet} successfully uploaded to the COS {curation_bucket} bucket')\n",
    "    except Exception as e:\n",
    "        print(f\"\\033[91mFailed upload file to the bucket {curation_bucket}. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
